---
layout: post
title:  "인공신경망 - Output Units"
date:   2020-04-10 18:00:08
categories: 수업
comments: true 
use_math: true
---
-----

<div style = "font-weight:500; font-size:1.0em; margin-left: 1em; margin-right: 1em;text-align:justify; ">
비용 함수의 선택은 출력 단위(output unit)의 선택과 밀접하게 연결된다. 대부분의 경우, 단순하게 데이터 분포와 모델 분포 사이의 cross-entropy를 사용한다. 우리는 feedforward 네트워크가 $h=f(x;\theta)$로 정의된 숨겨진 특징(hidden features)을 제공한다고 가정한다. 출력 계층(output layer)의 역할은 네트워크가 수행해야 하는 task를 완료하기 위하여 특징에 추가적인 변환을 제공하는 것이다.
<br><br>
<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Linear Units for Gaussian Output Distributions
</span>
<br><br>
간단한 출력 단위 중 하나는 비선형성(nonlinearity)이 없는 affine 변환에 기초한 것이다. 이것들은 주로 선형 단위로 불리며 특징 $h$가 주어질 때, 선형 출력 단위의 계층은 벡터 $\hat{y}=W^{\top} h+b$를 생성한다. 선형 출력 계층들은 조건부 가우시안 분포인 $p(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; \hat{\boldsymbol{y}}, \boldsymbol{I})$의 평균을 생성하는데 사용된다. 선형 단위는 포화 상태(saturate)가 아니기 때문에 기울기(gradient) 기반 최적화 알고리즘에는 별다른 어려움이 없으며 다양한 최적화 알고리즘과 함께 사용될 수 있다. 
<br><br>

<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Sigmoid Units for Bernoulli Output Distributions
</span>
<br><br>
많은 작업에서 이진 변수(binary variable)인 $y$값을 예측해야 한다. 베르누이 분포는 단 하나의 수로 정의되므로 신경망(neural net)은 $P(y=1|x)$만 예측하면 된다. 이 숫자가 유효한 확률이 되려면 [0,1]간격에 있어야 하고 이 제약을 만족하려면 신중하게 디자인 할 필요가 있다. 유효한 확률을 얻기 위해 선형 단위를 사용하고 그 값을 한계점(threshold)으로 사용한다고 다음과 같이 가정한다.<br>
<p align="center">
<br>
 $P(y=1 | \boldsymbol{x})=\max \left\{0, \min \left\{1, \boldsymbol{w}^{\top} \boldsymbol{h}+b\right\}\right\}$
 </p>
경사하강법(gradient decent)으로는 이를 매우 효과적으로 훈련시킬 수 없다. $w^{\top} h+b$가 단위 구간을 벗어나면 파라미터에 대한 모델의 아웃풋 기울기가 0이 되기 때문이다. 대신 모델이 잘못된 답을 줄 때마다 항상 강한(strong) 기울기를 보장하는 다른 접근법을 사용하는 것이 좋다. 시그모이드 출력 단위는 $\hat{y}=\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{h}+b\right)$로 정의되며 $\sigma$는 로지스틱 시그모이드 함수이다. 
<br><br>
시그모이드 출력 단위는 두 개의 구성 요소를 가지고 있다고 생각할 수 있다. 
<ol>
<li>$z=w^{\top} h+b$를 계산하기 위해 선형 레이어를 사용하는 것</li>
<li>$z$를 확률로 변환하기 위해 시그모이드 활성화(activation) 함수를 사용하는 것</li>
</ol>
만약 비정규화된(unnormalized) 로그 확률이 $y$와 $z$에서 선형이라고 가정한다면 비정규화된 확률을 얻기 위해 지수화(exponentiate)할 수 있다. $z$의 시그모이드 변환에 의해 베르누이 분포 산출을 위한 정규화 과정은 다음과 같다. 
<br><br>
<p align="center">
$\begin{aligned} 
\log \tilde{P}(y) &=y z \\ 
\tilde{P}(y) &=\exp (y z) \\ 
P(y) &=\frac{\exp (y z)}{\sum_{y^{\prime}=0}^{1} \exp \left(y^{\prime} z\right)} \\ 
P(y) &=\sigma((2 y-1) z) 
\end{aligned}$
</p>
로그 공간에서 확률을 예측하는 이 접근법은 최대 우도(maximum likelihood) 학습에 사용하는 것이 보통이다. 왜냐하면 최대 우도에 사용되는 비용 함수가 $ \log P(y|x)$이기 때문에, 비용함수에서 로그는 시그모이드의 exp를 없애준다. 이러한 효과가 없다면, 시그모이드의 포화상태는 gradient 기반 학습의 진전을 막았을 것이다. 
<br><br>
시그모이드에 의해 파라미터화되는 베르누이의 최대 우도 학습에 대한 손실 함수(loss function)는 다음과 같다. $\zeta$ 는 softplus 함수이다.
<br><br>
<p align="center">
$ \begin{aligned} 
J(\boldsymbol{\theta}) &=-\log P(y | \boldsymbol{x}) \\ 
&=-\log \sigma((2 y-1) z) \\ 
&=\zeta((1-2 y) z) 
\end{aligned}$
</p>
$(1-2y)z$가 very negative(매우 작은 음수)한 경우만 포화되므로 포화상태는 모델이 이미 올바른 정답을 가지고 있을 때만 발생한다. 즉, $y=1$ 이고 $z$가 very positive(매우 큰 양수) 하거나 $y=0$이고 $z$가 very negative 한 경우만 발생한다. $z$가 잘못된 사인을 가질 때, softplus 함수에 대한 인수(argument) $(1-2y)z$는 $|z|$로 단순화될 수 있다. 
<br><br>
<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Softmax Units for Multinoulli Output Distributions
</span>
<br><br>
$n$개의 값을 가지는 이산 변수의 경우로 일반화하기 위해 $\hat{y}_{i}=P(y=i | \boldsymbol{x})$에 대한 벡터 $\hat{y}$가 필요하다. 먼저 선형 계층이 비정규화된 로그 확률을 다음과 같이 예측한다.
 <p align="center">
<br>
 $z=W^{\top}h+b \quad $ where $z_{i}=\log \tilde{P}(y=i | \boldsymbol{x})$
 </p>
소프트맥스 함수는 원하는 $\hat{y}$를 얻기 위해 $z$를 지수화하고 정규화할 수 있다. 
<p align="center">
<br>
 $\operatorname{softmax}(\boldsymbol{z})_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}$
 </p>
이를 다음과 같이 최대화 하길 원한다. 
<p align="center">
<br>
 $ \begin{aligned} 
 \log P(\mathrm{y}=i ; z)&=\log \operatorname{softmax}(z)_{i} \\
 \log \operatorname{softmax}(\boldsymbol{z})_{i}&=z_{i}-\log \sum_{j} \exp \left(z_{j}\right)
 \end{aligned}
 $
 </p>
 두번째 항은 $\max_j z_j$로 근사될 수 있다. 만약 올바른 답이 소프트맥스에 대한 가장 큰 입력을 이미 가지고 있다면, $-z_i$ 항과 $\log \sum_{j} \exp \left(z_{j}\right) \approx \max _{j} z_{j}=z_{i}$ 항은 거의 취소된다. 이 예제는 전체 트레이닝 비용에는 기여하지 못하지만 아직 정확하게 분류되지 않은 다른 예제에 의해 지배될(dominated) 것이다. 
 <br><br>

신경과학적인(neuroscientific) 관점에서 소프트 맥스를 단위들 사이의 경쟁의 형태로 만든다는 생각은 흥미롭다. 
소프트맥스 아웃풋의 합은 항상 1이기 때문에 한 단위 값이 증가하면 다른 단위 값은 감소한다. 이는 피질(cortex) 안에 있는 근처 뉴런사이에 존재하는 것으로 여겨지는 측면 억제(lateral inhibition)와 유사하다. "소프트(soft)"라는 용어는 소프트맥스가 연속이고 미분가능하다는 사실에서 유래하였으며, 소프트맥스 함수는 argmax의 softened 버전을 제공한다. 
<br><br>

<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Other Output Types
</span>
<br><br>
신경망(neural network)은 우리가 원하는 거의 모든 종류의 출력계층을 일반화할 수 있다. 최대 우도 원리(principle)는 출력 계층에 대해 비용 함수를 좋게 설계하는 방법에 대한 가이드를 준다. 조건부 분포 $p(y|x;\theta)$를 정의한다면 최대우도원리는 비용 함수를 $\log p(y|x;\theta)$로 사용하도록 한다. 
<br><br>
일반적으로 신경망은 함수 $f(x;\theta)$를 표현하는 것으로 생각할 수 있다. 이 함수의 아웃풋은 $y$ 값을 직접 예측하는 것은 아니다. 대신 $f(x;\theta)=\omega$가 $y$ 에 대한 분포의 파라미터를 제공한다. 손실함수는 $-\log p(y;\omega(x))$로 해석될 수 있다. 
<br><br>
우리는 가끔 multimodal regression을 수행할 수도 있다. 이는 동일한 $x$값에 대해 y 스페이스에 여러개의 다른 peak들을 가질 수 있는 조건부 분포 $p(y|x)$에서 실제 값을 예측하는 것이다. 이 경우에는 Gaussian mixture를 아웃풋으로 하고 이런 신경망을 흔히 mixture density networks라고 한다. 
<p align="center">
<br>
$$
p(\boldsymbol{y} | \boldsymbol{x})=\sum_{i=1}^{n} p(\mathrm{c}=i | \boldsymbol{x}) \mathcal{N}\left(\boldsymbol{y} ; \boldsymbol{\mu}^{(i)}(\boldsymbol{x}), \boldsymbol{\Sigma}^{(i)}(\boldsymbol{x})\right)$$
</p>
그러나 조건부 가우시안 혼합의 gradient 기반 최적화는 신뢰할 수 없다. 왜냐하면 수치적으로 불안정한 분산에 의한 division을 얻을 수 있기 때문이다. 특정한 예시에서 일부 분산이 작은 경우 매우 큰 gradient를 산출한다. 가우시안 혼합 아웃풋은 특히 물리적인 객체의 음성과 움직임의 생성 모델에 효과적이다. 
<br><br>

<p align="center">
<img src="/images/post_img/AN2.png" width="650" height="300">
</p>
임의의 high capacity의 가장 극단적인 경우에 도달하기 위해 <b style = "color:#d7385e;font-size:1.2">비모수적(non-parametric)</b> 모델의 개념을 소개한다. 모수적(parametric) 모델은 어떤 데이터가 발견되기 전까지 유한하고 고정된 사이즈의 파라미터 벡터로 표현된 함수를 학습한다. 비모수적 모델은 이러한 제한이 없으며 최근접회귀(nearest neighbor regression)를 예로 들 수 있다. 이상적인 모델은 데이터를 생성하는 실제 확률 분포를 간단히 아는 oracle이다. 그러나 분포에 노이즈가 있기 때문에 약간의 에러가 발생할 수 있다. 실제 분포로 부터 예측하는 oracle에 의해 발생한 에러를 <b style = "color:#d7385e;font-size:1.2">베이즈 에러(Bayes error)</b>라고 한다. 
<br><br>

트레이닝 에러와 일반화 에러는 트레이닝셋의 크기에 따라 달라진다. 기대되는(expected) <b style = "color:#d7385e;font-size:1.2">일반화 에러는 트레이닝 example의 수가 증가한다고 증가할 수 없다.</b> 비모수적 모델에서는 데이터가 많을수록 가능한 가장 좋은 에러를 얻을 때까지 더 좋은 일반화를 만든다. 최적 capacity보다 작은 어떤 고정된 모수 모델은 베이즈 에러를 초과하는 에러값에 근접할 것이다. 모델은 최적의 capacity를 가질 수 있지만 여전히 트레이닝 에러와 일반화 에러 사이의 큰 차이를 가진다. 
<br><br>
<div style="border: 1px; float: right;margin-left: 1em; margin-right: 1em; " >
<img src="/images/post_img/AN4.png" width="290" height="250" >
</div>
<div style="border: 1px; margin-left: 2em; margin-right: 1em; ">
<img src="/images/post_img/AN3.png" width="290" height="250">
</div>
<br><br>

<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
No Free Lunch Theorem
</span>
<br><br>
'공짜 점심은 없다' 라는 이론은 머신러닝 알고리즘이 다른 어떤 것보다 일반적으로 낫지 않음을 뜻한다. 즉, 머신러닝이 만능은 아니라는 것이다. 모든 분류 알고리즘은 이전에 관찰되지 않은 point를 분류할 때, 가능한 모든 데이터 생성 분포에 대한 평균으로써 <b style = "color:#d7385e;font-size:1.2">동일한 오류 비율</b>을 가진다. 다행히도 이런 결과는 가능한 모든 데이터 생성 분포에 대해 평균을 낼 때만 유지되므로 실제 응용사례에서 접하는 확률 분포의 종류에 대한 가정을 만들면, 이러한 분포에서 잘 수행되는 학습 알고리즘을 설계할 수 있다. 
<br><br>
머신러닝 연구의 목표는 일반적인 학습 알고리즘이나 절대적으로 가장 좋은 학습 알고리즘을 찾는 것이 아니다. 대신, AI 에이전트가 경험하는 실제 세계와 어떤 분포가 관련되는지 이해하고 우리가 <b style = "color:#d7385e;font-size:1.2">관심을 갖는 분포</b>에서 도출된 데이터로부터 <b style = "color:#d7385e;font-size:1.2">어떤 종류의 머신러닝 알고리즘이 잘 수행되는지 파악하는 것</b>이다. 
<br><br>

<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
정규화 (Regularization)
</span>
<br><br>
No Free Lunch Theorem은 머신러닝 알고리즘이 구체적인 task를 잘 수행하도록 디자인 해야함을 시사한다. 지금까지 학습 알고리즘을 수정하는 유일한 방법은 솔루션의 가설 공간에 참수를 추가하거나 제거함으로써 모델의 표현 용량을 증가시키거나 감소시키는 것이었다. 어떤 종류의 함수에서 솔루션을 도출할 수 있게 선택하고 이러한 함수의 양을 조절함으로써 알고리즘의 성능을 제어할 수 있다. 
<br><br>

또한 가설 공간에서 다른곳으로 하나의 솔루션에 대한 선호도(preference)를 줄 수 있다. 예를 들어 <b style = "color:#d7385e;font-size:1.2">weight decay</b> ($J(w)=MSE_{trin} + \lambda w^Tw$) 가 있다. 트레이닝 데이터 적합에 대한 가중치를 선택할 수 있는 것인데 <b style = "color:#d7385e;font-size:1.2">weight decay의 값이 커질수록</b> 가중치 값이 작아져 <b style = "color:#d7385e;font-size:1.2">과적합 현상을 해소</b>할 수 있지만, weight decay 값을 너무 크게 하면 저적합 현상이 발생하므로 적당한 값을 사용해야 한다. 좀더 일반적으로 함수 $f(x;\theta)$ 를 비용함수에 정규화(regularizer)라 불리는 패널티로 추가해줌으로써 모델을 정규화할 수 있다. weight decay의 경우 regularizer는 $\Omega (w) = w^Tw$ 이다. 
<br><br>
<p align="center">
<img src="/images/post_img/AN5.png" width="650" height="300">
</p>

다른 솔루션에 대한 선호도를 표현하는 방법은 암시적으로(implicily)나 명백하게(explicitly) 많이 있다. 이와 같이 다른 접근방법들은 <b style = "color:#d7385e;font-size:1.2">정규화(regularization)</b>로 알려져 있다. 정규화는 트레이닝 에러가 아닌 <b style = "color:#d7385e;font-size:1.2">일반화 에러를 줄이기 위한 학습 알고리즘</b>이며 머신러닝의 핵심 관심사 중 하나이다. 앞서 살펴본 공짜 점심은 없다는 이론은 최고의(best) 머신러닝 알고리즘은 없으며, 특히 가장 좋은 형태의 정규화는 없다는 것을 확실하게 보여준다. 